{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlLRjWa5dstA",
        "outputId": "a63b46b3-3e24-461d-c11d-ac8653695eb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'I really enjoyed this!' → Positive 😊\n",
            "'That was awful and boring.' → Positive 😊\n"
          ]
        }
      ],
      "source": [
        "# Sentiment Classification using Traditional NLP (TF-IDF + Logistic Regression)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# 1. Example dataset (10 sentences)\n",
        "sentences = [\n",
        "    \"I love this product, it works great!\",\n",
        "    \"This is the worst experience I’ve ever had.\",\n",
        "    \"Absolutely fantastic! Highly recommend it.\",\n",
        "    \"I hate how slow and buggy this is.\",\n",
        "    \"The movie was amazing and full of surprises.\",\n",
        "    \"Terrible service, I will never come back.\",\n",
        "    \"The food was delicious and well presented.\",\n",
        "    \"I’m disappointed, the quality was poor.\",\n",
        "    \"Excellent performance, I’m really impressed.\",\n",
        "    \"Not worth the money, very bad quality.\"\n",
        "]\n",
        "\n",
        "# Corresponding labels (1 = positive, 0 = negative)\n",
        "labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
        "\n",
        "# 2. Create TF-IDF + Logistic Regression pipeline\n",
        "model = make_pipeline(TfidfVectorizer(), LogisticRegression())\n",
        "\n",
        "# 3. Train the model\n",
        "model.fit(sentences, labels)\n",
        "\n",
        "# 4. Test on new examples\n",
        "test_sentences = [\n",
        "    \"I really enjoyed this!\",\n",
        "    \"That was awful and boring.\"\n",
        "]\n",
        "\n",
        "predictions = model.predict(test_sentences)\n",
        "\n",
        "# 5. Show results\n",
        "for text, pred in zip(test_sentences, predictions):\n",
        "    sentiment = \"Positive 😊\" if pred == 1 else \"Negative 😠\"\n",
        "    print(f\"'{text}' → {sentiment}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s9roQfDJeQEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb450f89"
      },
      "source": [
        "Here's an explanation of the roles of `TfidfVectorizer` and `LogisticRegression` in the provided code:\n",
        "\n",
        "**`TfidfVectorizer`:**\n",
        "\n",
        "*   **Job:** The `TfidfVectorizer` is responsible for converting the text data (the sentences) into a numerical representation that the machine learning model can understand. It does this by calculating the TF-IDF score for each word in each document (sentence).\n",
        "*   **TF-IDF:** TF-IDF stands for Term Frequency-Inverse Document Frequency.\n",
        "    *   **Term Frequency (TF):** Measures how often a word appears in a document.\n",
        "    *   **Inverse Document Frequency (IDF):** Measures how rare a word is across all documents. Words that appear in many documents (like \"the\" or \"a\") have a lower IDF, while words that are specific to a few documents have a higher IDF.\n",
        "*   **Output:** The output of the `TfidfVectorizer` is a sparse matrix where each row represents a sentence and each column represents a unique word from the entire vocabulary. The values in the matrix are the TF-IDF scores for each word in each sentence. This matrix is essentially a numerical representation of the text data, capturing the importance of words in each sentence relative to the entire dataset.\n",
        "\n",
        "**`LogisticRegression`:**\n",
        "\n",
        "*   **Job:** `LogisticRegression` is a linear model used for binary classification (in this case, classifying sentiment as either positive or negative). It takes the numerical representation of the text (the TF-IDF matrix) as input and learns to predict the probability of a sentence belonging to a particular class (positive or negative).\n",
        "*   **How it uses the `TfidfVectorizer` output:** The TF-IDF matrix generated by `TfidfVectorizer` serves as the input features for the `LogisticRegression` model. Each column in the matrix (representing a word's TF-IDF score) becomes a feature that the logistic regression model uses to learn the relationship between the words and the sentiment labels. The model learns weights for each word feature, determining how much that word contributes to a positive or negative sentiment prediction.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "The `TfidfVectorizer` transforms the raw text into a numerical format that highlights the importance of words in each sentence. This numerical representation is then fed into the `LogisticRegression` model, which uses these features to learn a decision boundary that can classify new sentences as either positive or negative based on the patterns it learned from the training data. The `make_pipeline` function chains these two steps together, creating a single model that takes raw text as input and outputs sentiment predictions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Example data\n",
        "texts = [\n",
        "    \"I love this phone\", \"I hate this product\",\n",
        "    \"Absolutely amazing\", \"Very bad experience\",\n",
        "    \"Fantastic quality\", \"Terrible item\"\n",
        "]\n",
        "labels = [1, 0, 1, 0, 1, 0]  # 1 = Positive, 0 = Negative\n",
        "\n",
        "# Create pipeline\n",
        "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
        "model.fit(texts, labels)\n",
        "\n",
        "# Test\n",
        "test_sentences = [\"This is awesome!\", \"Worst ever!\"]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    prediction = model.predict([sentence])[0]\n",
        "    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
        "    print(f\"'{sentence}' → {sentiment}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9eNF70oe42X",
        "outputId": "7d3856cd-b418-4085-da54-720d98d8d875"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'This is awesome!' → Positive\n",
            "'Worst ever!' → Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xcPKOffYfT9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "# Example data\n",
        "texts = [\n",
        "    \"I love this movie\", \"This is awful\",\n",
        "    \"Amazing story\", \"Very bad acting\",\n",
        "    \"Fantastic direction\", \"Terrible sound\"\n",
        "]\n",
        "labels = [1, 0, 1, 0, 1, 0]\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "X = tokenizer.texts_to_sequences(texts)\n",
        "X = pad_sequences(X, maxlen=5)\n",
        "y = np.array(labels) # Convert labels to a numpy array\n",
        "\n",
        "# Model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=5),\n",
        "    LSTM(16),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=20, verbose=0)\n",
        "\n",
        "# Test\n",
        "test = tokenizer.texts_to_sequences([\"I really enjoyed this film\"])\n",
        "test = pad_sequences(test, maxlen=5)\n",
        "prediction_probability = model.predict(test)[0][0] # Get the single probability value\n",
        "sentiment = \"Positive\" if prediction_probability > 0.5 else \"Negative\"\n",
        "print(f\"'{'I really enjoyed this film'}' → {sentiment} (Probability: {prediction_probability:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pplWwQ0Ue-xl",
        "outputId": "86b196b6-5acf-4913-9b43-f8490e103bef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step\n",
            "'I really enjoyed this film' → Positive (Probability: 0.5030)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-wUcfA3pfqgG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}