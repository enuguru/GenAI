{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-DXBFrotCQJ"
      },
      "source": [
        "# Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kX8WYcJtCQK"
      },
      "source": [
        "- Create the traditinal ngram-based language model\n",
        "- Codes from [A comprehensive guide to build your own language model in python](https://medium.com/analytics-vidhya/a-comprehensive-guide-to-build-your-own-language-model-in-python-5141b3917d6d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHk-AIKFtCQK"
      },
      "source": [
        "## Training a Trigram Language Model using Reuters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "BAJOFiM8CpWw",
        "outputId": "80b00bfd-ca6c-44e1-9276-dd8a1bf43e34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bxN7Dh4DtCQL",
        "outputId": "6eb61dc9-e1c4-46cc-9a41-92abf79afb9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 12.1 s, sys: 918 ms, total: 13 s\n",
            "Wall time: 13.1 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# code courtesy of https://nlpforhackers.io/language-models/\n",
        "\n",
        "from nltk.corpus import reuters\n",
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# Create a placeholder for model\n",
        "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "# Count frequency of co-occurance\n",
        "for sentence in reuters.sents():\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        model[(w1, w2)][w3] += 1\n",
        "\n",
        "# Let's transform the counts to probabilities\n",
        "for w1_w2 in model:\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAh1Fy-4tCQL"
      },
      "source": [
        "## Check Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ts30A1w8tCQL",
        "outputId": "7e7e97c5-132e-4110-efd6-44173f668fa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('conference', 0.25),\n",
              " ('of', 0.125),\n",
              " ('.', 0.125),\n",
              " ('with', 0.08333333333333333),\n",
              " (',', 0.08333333333333333),\n",
              " ('agency', 0.08333333333333333),\n",
              " ('that', 0.08333333333333333),\n",
              " ('brought', 0.041666666666666664),\n",
              " ('about', 0.041666666666666664),\n",
              " ('broke', 0.041666666666666664),\n",
              " ('on', 0.041666666666666664)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "sorted(dict(model[\"the\",\"news\"]).items(), key=lambda x:-1*x[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhSjwIL8tCQL"
      },
      "source": [
        "## Text Generation Using the Trigram Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak5qcAHqtCQM"
      },
      "source": [
        "- Using the trigram model to predict the next word.\n",
        "- The prediction is based on the predicted probability distribution of the next words: words above a predefined cut-off are randomly selected.\n",
        "- The text generator ends when two consecutuve None's are predicted (signaling the end of the sentence)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dnQ8OiCntCQM",
        "outputId": "f055f8a5-a860-46f6-82e2-12c5b78e8816",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the news agency INA reported .\n"
          ]
        }
      ],
      "source": [
        "# code courtesy of https://nlpforhackers.io/language-models/\n",
        "import random\n",
        "\n",
        "# starting words\n",
        "text = [\"the\", \"news\"]\n",
        "sentence_finished = False\n",
        "\n",
        "while not sentence_finished:\n",
        "  # select a random probability threshold\n",
        "  r = random.random()\n",
        "  accumulator = .0\n",
        "\n",
        "  for word in model[tuple(text[-2:])].keys():\n",
        "      accumulator += model[tuple(text[-2:])][word]\n",
        "      # select words that are above the probability threshold\n",
        "      if accumulator >= r:\n",
        "          text.append(word)\n",
        "          break\n",
        "\n",
        "  if text[-2:] == [None, None]:\n",
        "      sentence_finished = True\n",
        "\n",
        "print (' '.join([t for t in text if t]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcit8ozktCQM"
      },
      "source": [
        "## Issues of Ngram Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHjpX0ZEtCQM"
      },
      "source": [
        "- The ngram size is of key importance. The higher the order of the ngram, the better the prediction. But it comes with the issues of computation overload and data sparceness.\n",
        "- Unseen ngrams are always a concern.\n",
        "- Probability smoothing issues.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thYrI7BOtCQM"
      },
      "source": [
        "## Neural Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNhJOYI5tCQM"
      },
      "source": [
        "- Neural language model based on deep learning may provide a better alternative to model the probabilistic relationships of linguistic units."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python-notes",
      "language": "python",
      "name": "python-notes"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}