{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHRfZcts4JY8",
        "outputId": "1755192a-8535-46e9-8c8d-84e824b87621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Q-values:\n",
            "[[[0.53144099 0.58880048 0.531441   0.59049   ]\n",
            "  [0.59049    0.65548617 0.531441   0.6561    ]\n",
            "  [0.6561     0.72899998 0.59049    0.729     ]\n",
            "  [0.72899999 0.81       0.6561     0.72899999]]\n",
            "\n",
            " [[0.40992949 0.30520302 0.43292838 0.65597117]\n",
            "  [0.59049    0.64521947 0.54448622 0.72899907]\n",
            "  [0.63371818 0.76128498 0.60086144 0.81      ]\n",
            "  [0.729      0.9        0.729      0.81      ]]\n",
            "\n",
            " [[0.49125071 0.00730628 0.03697619 0.06985335]\n",
            "  [0.23244184 0.03552389 0.09077878 0.80051481]\n",
            "  [0.62383582 0.55591331 0.53722415 0.9       ]\n",
            "  [0.81       1.         0.80999994 0.89999999]]\n",
            "\n",
            " [[0.11636897 0.         0.         0.        ]\n",
            "  [0.24365956 0.00733798 0.01164024 0.        ]\n",
            "  [0.29908555 0.11152262 0.01381338 0.86491483]\n",
            "  [0.         0.         0.         0.        ]]]\n",
            "\n",
            "Path taken by the agent:\n",
            "[(0, 0), (0, 1), (0, 2), (0, 3), (1, 3), (2, 3), (3, 3)]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# -----------------------------\n",
        "# Simple GridWorld Environment\n",
        "# -----------------------------\n",
        "class GridWorld:\n",
        "    def __init__(self, size=4):\n",
        "        self.size = size\n",
        "        self.start = (0, 0)\n",
        "        self.goal = (size - 1, size - 1)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.pos = self.start\n",
        "        return self.pos\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = self.pos\n",
        "\n",
        "        if action == 0:   # up\n",
        "            x = max(0, x - 1)\n",
        "        elif action == 1: # down\n",
        "            x = min(self.size - 1, x + 1)\n",
        "        elif action == 2: # left\n",
        "            y = max(0, y - 1)\n",
        "        elif action == 3: # right\n",
        "            y = min(self.size - 1, y + 1)\n",
        "\n",
        "        self.pos = (x, y)\n",
        "\n",
        "        # Reward\n",
        "        if self.pos == self.goal:\n",
        "            return self.pos, 1, True  # reached goal\n",
        "        else:\n",
        "            return self.pos, 0, False  # no reward\n",
        "\n",
        "# -----------------------------\n",
        "# Q-Learning Algorithm\n",
        "# -----------------------------\n",
        "env = GridWorld(size=4)\n",
        "\n",
        "# Q-table: 4x4 grid with 4 actions each\n",
        "Q = np.zeros((4, 4, 4))\n",
        "\n",
        "alpha = 0.1      # learning rate\n",
        "gamma = 0.9      # discount factor\n",
        "epsilon = 0.3    # exploration probability\n",
        "episodes = 2000\n",
        "\n",
        "for _ in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        x, y = state\n",
        "\n",
        "        # Epsilon-greedy action selection\n",
        "        if random.random() < epsilon:\n",
        "            action = random.choice([0, 1, 2, 3])  # explore\n",
        "        else:\n",
        "            action = np.argmax(Q[x, y])           # exploit\n",
        "\n",
        "        next_state, reward, done = env.step(action)\n",
        "        nx, ny = next_state\n",
        "\n",
        "        # Q-learning update rule\n",
        "        Q[x, y, action] = Q[x, y, action] + alpha * (\n",
        "            reward + gamma * np.max(Q[nx, ny]) - Q[x, y, action]\n",
        "        )\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "# -----------------------------\n",
        "# Test the trained agent\n",
        "# -----------------------------\n",
        "state = env.reset()\n",
        "done = False\n",
        "path = [state]\n",
        "\n",
        "print(\"Learned Q-values:\")\n",
        "print(Q)\n",
        "\n",
        "while not done:\n",
        "    x, y = state\n",
        "    action = np.argmax(Q[x, y])\n",
        "    state, reward, done = env.step(action)\n",
        "    path.append(state)\n",
        "\n",
        "print(\"\\nPath taken by the agent:\")\n",
        "print(path)\n"
      ]
    }
  ]
}