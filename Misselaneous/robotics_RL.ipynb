{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n"
      ],
      "metadata": {
        "id": "OO1T9AZd_2Nn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------------\n",
        "# Robot Environment\n",
        "# ------------------------\n",
        "class RobotEnv:\n",
        "    def __init__(self, size=5):\n",
        "        self.size = size\n",
        "        self.start = (0, 0)\n",
        "        self.goal = (4, 4)\n",
        "        self.obstacle = (2, 2)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.pos = self.start\n",
        "        return self.pos\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = self.pos\n",
        "\n",
        "        # Actions: 0=up, 1=down, 2=left, 3=right\n",
        "        if action == 0: x = max(0, x - 1)\n",
        "        if action == 1: x = min(self.size - 1, x + 1)\n",
        "        if action == 2: y = max(0, y - 1)\n",
        "        if action == 3: y = min(self.size - 1, y + 1)\n",
        "\n",
        "        self.pos = (x, y)\n",
        "\n",
        "        if self.pos == self.goal:\n",
        "            return self.pos, 10, True\n",
        "        elif self.pos == self.obstacle:\n",
        "            return self.pos, -5, False\n",
        "        else:\n",
        "            return self.pos, -1, False"
      ],
      "metadata": {
        "id": "LgjnMgt8_3Oh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------\n",
        "# Q-Learning\n",
        "# ------------------------\n",
        "env = RobotEnv()\n",
        "Q = np.zeros((5, 5, 4))\n",
        "\n",
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "epsilon = 0.3\n",
        "episodes = 2000\n",
        "MAX_STEPS = 50   # <-- IMPORTANT FIX\n",
        "\n",
        "for _ in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    for step in range(MAX_STEPS):    # <-- Prevent infinite loops\n",
        "        x, y = state\n",
        "\n",
        "        # Epsilon-greedy\n",
        "        if random.random() < epsilon:\n",
        "            action = random.randint(0, 3)\n",
        "        else:\n",
        "            action = np.argmax(Q[x, y])\n",
        "\n",
        "        next_state, reward, done = env.step(action)\n",
        "        nx, ny = next_state\n",
        "\n",
        "        # Update Q-value\n",
        "        Q[x, y, action] += alpha * (\n",
        "            reward + gamma * np.max(Q[nx, ny]) - Q[x, y, action]\n",
        "        )\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n"
      ],
      "metadata": {
        "id": "5mpadmss_8vF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cybIjIpy3eVA",
        "outputId": "fe916e14-292f-454e-f03e-927da4200f94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Q-values:\n",
            "[[[-1.39065582 -0.43406231 -1.39065581 -0.434062  ]\n",
            "  [-0.4340621   0.62881827 -1.39065584  0.62882   ]\n",
            "  [ 0.62881976  1.8098     -0.43406286  1.80979985]\n",
            "  [ 1.61474061  3.122       0.44875564  2.77923595]\n",
            "  [ 0.70796055  4.57193866  0.72450409  1.37982998]]\n",
            "\n",
            " [[-1.46181154 -1.21039337 -0.83238872  0.62881999]\n",
            "  [-0.47997571  1.32379574 -0.45431471  1.8098    ]\n",
            "  [ 0.62881989 -0.8780001   0.62881997  3.122     ]\n",
            "  [ 1.80979944  4.58        1.80979997  4.57999998]\n",
            "  [ 2.91538254  6.2         3.0422935   4.40749729]]\n",
            "\n",
            " [[-1.74757095 -1.80233902 -1.6051075   0.99082361]\n",
            "  [-0.81069234  3.03217112 -1.42269484 -1.75500186]\n",
            "  [ 1.55035614  4.22609884  0.56882622  4.58      ]\n",
            "  [ 3.12199998  6.19999981 -0.87800016  6.2       ]\n",
            "  [ 4.57999999  8.          4.58        6.19999999]]\n",
            "\n",
            " [[-1.22997316 -1.15077802 -1.33570736  1.17696124]\n",
            "  [-1.00756789  4.55677847 -0.99523064  2.17379054]\n",
            "  [-1.32487744  6.19306694  0.75140027  3.15476075]\n",
            "  [ 4.46461676  8.          4.07788308  7.66529908]\n",
            "  [ 6.19999996 10.          6.19999936  7.99999982]]\n",
            "\n",
            " [[-0.83251227 -0.91186144 -0.76825626  1.71031136]\n",
            "  [ 0.36552788  2.03801223 -0.56864275  6.19815714]\n",
            "  [ 2.87941798  4.81529693  3.01037514  7.99999976]\n",
            "  [ 6.10439204  7.87175374  6.04579593 10.        ]\n",
            "  [ 0.          0.          0.          0.        ]]]\n",
            "\n",
            "Robot’s optimal learned path:\n",
            "[(0, 0), (0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (2, 4), (3, 4), (4, 4)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ------------------------\n",
        "# Test the trained robot\n",
        "# ------------------------\n",
        "state = env.reset()\n",
        "path = [state]\n",
        "done = False\n",
        "\n",
        "for step in range(MAX_STEPS):  # <-- avoid infinite loops during testing too\n",
        "    x, y = state\n",
        "    action = np.argmax(Q[x, y])\n",
        "    state, reward, done = env.step(action)\n",
        "    path.append(state)\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(\"Learned Q-values:\")\n",
        "print(Q)\n",
        "\n",
        "print(\"\\nRobot’s optimal learned path:\")\n",
        "print(path)\n"
      ]
    }
  ]
}