{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "edc023f19e934f40a922cdebc6487ee0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aca7b3e12cd2484aa67a82dbcbb5d5db",
              "IPY_MODEL_3c86edbe8d2b4df2bb4a335a180849a6",
              "IPY_MODEL_16e33ef0173f46beae81c5c9a5809fd9"
            ],
            "layout": "IPY_MODEL_b4d16a53bd7f443f9d858868b5e12655"
          }
        },
        "aca7b3e12cd2484aa67a82dbcbb5d5db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14b1ca96ad7b4e3f92a197a809cc58e8",
            "placeholder": "​",
            "style": "IPY_MODEL_ee36dbd2fa7d437dbf458b600f709188",
            "value": "Map: 100%"
          }
        },
        "3c86edbe8d2b4df2bb4a335a180849a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c9a612c6d4f4066a77a250f8325d5ce",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5523a54943ab427f81f5ca833fd65f3a",
            "value": 8
          }
        },
        "16e33ef0173f46beae81c5c9a5809fd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2a81b0b31ce45d9a0a785c766ba3733",
            "placeholder": "​",
            "style": "IPY_MODEL_bf5213a36d3f417c8820d28218b09b8c",
            "value": " 8/8 [00:00&lt;00:00, 419.51 examples/s]"
          }
        },
        "b4d16a53bd7f443f9d858868b5e12655": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14b1ca96ad7b4e3f92a197a809cc58e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee36dbd2fa7d437dbf458b600f709188": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c9a612c6d4f4066a77a250f8325d5ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5523a54943ab427f81f5ca833fd65f3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e2a81b0b31ce45d9a0a785c766ba3733": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf5213a36d3f417c8820d28218b09b8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import Dataset\n",
        "import json"
      ],
      "metadata": {
        "id": "m1f14iFAQLCo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample training data - replace with your own dataset\n",
        "sample_data = [\n",
        "    \"The weather today is beautiful and sunny.\",\n",
        "    \"Machine learning is revolutionizing technology.\",\n",
        "    \"Python is a versatile programming language.\",\n",
        "    \"Fine-tuning models requires careful preparation.\",\n",
        "    \"Natural language processing has many applications.\",\n",
        "    \"Deep learning models need quality training data.\",\n",
        "    \"Transformers have changed how we approach NLP.\",\n",
        "    \"Text generation can be improved with fine-tuning.\"\n",
        "]"
      ],
      "metadata": {
        "id": "zPvm9uE0QNeI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(texts, tokenizer, max_length=128):\n",
        "    \"\"\"\n",
        "    Prepare the dataset for training\n",
        "    \"\"\"\n",
        "    def tokenize_function(examples):\n",
        "        # Tokenize the texts\n",
        "        tokenized = tokenizer(\n",
        "            examples['text'],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        # For language modeling, labels are the same as input_ids\n",
        "        tokenized['labels'] = tokenized['input_ids'].clone()\n",
        "        return tokenized\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = Dataset.from_dict({'text': texts})\n",
        "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    return tokenized_dataset"
      ],
      "metadata": {
        "id": "8o_25KZ4QQ8w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "q56yhyp3QHKq"
      },
      "outputs": [],
      "source": [
        "# Fine-tune the model: The fine_tune_model function loads a\n",
        "# pre-trained GPT-2 model and its corresponding tokenizer using\n",
        "# GPT2LMHeadModel.from_pretrained and GPT2Tokenizer.from_pretrained.\n",
        "# It then prepares the dataset using the function defined earlier.\n",
        "\n",
        "def fine_tune_model():\n",
        "    \"\"\"\n",
        "    Main function to fine-tune the model\n",
        "    \"\"\"\n",
        "    # Initialize model and tokenizer\n",
        "    model_name = \"gpt2\"  # You can change this to other models like \"distilgpt2\"\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "    # Add padding token if it doesn't exist\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Prepare dataset\n",
        "    print(\"Preparing dataset...\")\n",
        "    train_dataset = prepare_dataset(sample_data, tokenizer)\n",
        "\n",
        "    # Data collator for language modeling\n",
        "    # DataCollatorForLanguageModeling is used to format the data batches\n",
        "    # for training. TrainingArguments defines the training configuration,\n",
        "    # such as output directory, number of epochs, batch size,\n",
        "    # and logging settings.\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False,  # We're not doing masked language modeling\n",
        "    )\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./fine_tuned_model\",\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,\n",
        "        warmup_steps=10,\n",
        "        logging_steps=10,\n",
        "        save_steps=100,\n",
        "        eval_strategy=\"no\",  # Updated argument\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=False,\n",
        "        report_to=None,  # Disable wandb logging\n",
        "        logging_dir=None,\n",
        "    )\n",
        "\n",
        "    # Initialize trainer\n",
        "    # The Trainer class is the core component for training; it takes the model,\n",
        "    # training arguments, dataset, data collator, and tokenizer.\n",
        "    # The trainer.train() method starts the fine-tuning process.\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    # Start training\n",
        "\n",
        "    print(\"Starting fine-tuning...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the fine-tuned model\n",
        "    print(\"Saving fine-tuned model...\")\n",
        "    trainer.save_model(\"./fine_tuned_model\")\n",
        "    tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
        "\n",
        "    print(\"Fine-tuning completed!\")\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, tokenizer):\n",
        "    \"\"\"\n",
        "    Test the fine-tuned model with sample generation\n",
        "    \"\"\"\n",
        "    print(\"\\nTesting fine-tuned model:\")\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Get the device of the model\n",
        "    device = model.device\n",
        "\n",
        "    test_prompts = [\n",
        "        \"The weather today\",\n",
        "        \"Machine learning\",\n",
        "        \"Python programming\"\n",
        "    ]\n",
        "\n",
        "    for prompt in test_prompts:\n",
        "        # Encode the prompt and move to the model's device\n",
        "        input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "        # Generate text\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                input_ids,\n",
        "                max_length=50,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.8,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Decode and print\n",
        "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        print(f\"Prompt: '{prompt}'\")\n",
        "        print(f\"Generated: '{generated_text}'\")\n",
        "        print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "ruJwocHoQanj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and test saved model: The load_and_test_saved_model function\n",
        "# demonstrates how to load a previously saved fine-tuned model and\n",
        "# tokenizer using GPT2LMHeadModel.from_pretrained and GPT2Tokenizer.\n",
        "# from_pretrained from the saved directory.\n",
        "# It then calls test_model to evaluate the loaded model's performance.\n",
        "\n",
        "def load_and_test_saved_model():\n",
        "    \"\"\"\n",
        "    Load the saved fine-tuned model and test it\n",
        "    \"\"\"\n",
        "    print(\"\\nLoading saved fine-tuned model...\")\n",
        "\n",
        "    # Load the fine-tuned model and tokenizer\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"./fine_tuned_model\")\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"./fine_tuned_model\")\n",
        "\n",
        "    # Test the loaded model\n",
        "    test_model(model, tokenizer)"
      ],
      "metadata": {
        "id": "7CaRwMeBQhsh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e1UIesLRe69",
        "outputId": "e7d8d38a-8c0b-4c14-a741-3da5f5c82596"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Disable wandb logging explicitly\n",
        "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "    # Check if CUDA is available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    try:\n",
        "        # Fine-tune the model\n",
        "        model, tokenizer = fine_tune_model()\n",
        "\n",
        "        # Test the fine-tuned model\n",
        "        test_model(model, tokenizer)\n",
        "\n",
        "        # Demonstrate loading the saved model\n",
        "        load_and_test_saved_model()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        print(\"Make sure you have the required packages installed:\")\n",
        "        print(\"pip install torch transformers datasets\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 945,
          "referenced_widgets": [
            "edc023f19e934f40a922cdebc6487ee0",
            "aca7b3e12cd2484aa67a82dbcbb5d5db",
            "3c86edbe8d2b4df2bb4a335a180849a6",
            "16e33ef0173f46beae81c5c9a5809fd9",
            "b4d16a53bd7f443f9d858868b5e12655",
            "14b1ca96ad7b4e3f92a197a809cc58e8",
            "ee36dbd2fa7d437dbf458b600f709188",
            "4c9a612c6d4f4066a77a250f8325d5ce",
            "5523a54943ab427f81f5ca833fd65f3a",
            "e2a81b0b31ce45d9a0a785c766ba3733",
            "bf5213a36d3f417c8820d28218b09b8c"
          ]
        },
        "id": "kjG3iOSoQfEH",
        "outputId": "0c80a481-05dc-4088-bb35-95082dfcbca1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading model: gpt2\n",
            "Preparing dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "edc023f19e934f40a922cdebc6487ee0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3309506829.py:56: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12/12 00:13, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.714600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving fine-tuned model...\n",
            "Fine-tuning completed!\n",
            "\n",
            "Testing fine-tuned model:\n",
            "Prompt: 'The weather today'\n",
            "Generated: 'The weather today is good. That's the only one we know of. If you need a forecast for Friday night you can look at the weather forecast today on Weather Underground.\n",
            "\n",
            "\n",
            "The latest weather reports can be viewed using the weather app on Apple'\n",
            "--------------------------------------------------\n",
            "Prompt: 'Machine learning'\n",
            "Generated: 'Machine learning is an open-source, collaborative, collaborative science. It is a process that enables developers to collaborate with one another. This process is called \"collaboration\".\n",
            "\n",
            "It is one of the fundamental tenets of Artificial Intelligence. The idea'\n",
            "--------------------------------------------------\n",
            "Prompt: 'Python programming'\n",
            "Generated: 'Python programming is one of the most powerful technologies ever invented. It has changed, but so does our perceptions of what constitutes a programming language. We must look at both the actual design and implementation of programming languages.\n",
            "\n",
            "The design of programming languages is'\n",
            "--------------------------------------------------\n",
            "\n",
            "Loading saved fine-tuned model...\n",
            "\n",
            "Testing fine-tuned model:\n",
            "Prompt: 'The weather today'\n",
            "Generated: 'The weather today is nice, sunny, and breezy. But tonight I'm going to have to get used to the heat. In the morning I'm going to warm up the house, make sure my clothes are comfortable, and enjoy the day.'\n",
            "--------------------------------------------------\n",
            "Prompt: 'Machine learning'\n",
            "Generated: 'Machine learning can be used in new ways. For instance, computers can be trained to recognize faces and solve problems. A more sophisticated solution could be to combine neural networks to solve complex questions, such as problem solving.\n",
            "\n",
            "The field of computers is'\n",
            "--------------------------------------------------\n",
            "Prompt: 'Python programming'\n",
            "Generated: 'Python programming is challenging. With many programming languages, you need to learn how to program effectively. This is a topic that has been recently discussed at conferences and conferences.\n",
            "\n",
            "Programming in Python has been challenging but very rewarding. Here are some of'\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}