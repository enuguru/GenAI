{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jOxHsozL5Ck",
        "outputId": "a6568338-8252-40af-e82e-28fc4412104f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: In the beginning, there was only darkness.\n",
            "Generated Text: In the beginning, there was only darkness. Those who were born to men and women who were born to women, and those who were born to men and women who were born to women, and those who were born to women, and those who were born to women, and those who were born to women, and those who were born to women, and those who were born to women, and those who were born to women, and those who were born to women, and those who were born to women, and those who were born to women, and those who were born to women, and those who were born to women, and those who were born to women, and those that were born to women, and those who were born to women, and those who were born to women, and those who were born to women, and those who were born to women, and those who were born to women, and those who were born to women, and those who were born to women, and those who were born to women, and those who were born to women, and those who were born to women, and those who were born to women, and those who were born to women, and those who were born to women, and those who were born to women, and those who were born to women, and those who\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: The world of artificial intelligence is evolving rapidly.\n",
            "Generated Text: The world of artificial intelligence is evolving rapidly. It’s only the beginning.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The world of artificial intelligence is evolving rapidly. It’s only the beginning.\n",
            "\n",
            "When you look at the big picture, the big picture was not so much about a specific algorithm for the algorithm. It was more about how the algorithms were changing and how it was being implemented.\n",
            "In the late 1990s, we were studying large data sets that allowed us to analyze different kinds of data. We developed the new AI and applied it in a big way. But as a result, we were unable to develop a lot of highly effective algorithms that could solve problems in a very large way.\n",
            "We found that the new AI was different from the old ones because all the data in the data was different from the ones in the data.\n",
            "In addition to the new AI, we developed the new AI because the algorithms were different from the ones in the data.\n",
            "In the late 1990s, we were studying large data sets that allowed us to analyze different kinds of data. We developed the new AI because the algorithms were different from the ones in the data.\n",
            "In the late 1990s, we were studying large data sets that allowed us to analyze different kinds of data. We developed the new AI because\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: A quick brown fox jumps over the lazy dog.\n",
            "Generated Text: A quick brown fox jumps over the lazy dog.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Prompt: Space exploration is the next frontier for humanity.\n",
            "Generated Text: Space exploration is the next frontier for humanity. A new frontier is also being explored.\n",
            "\n",
            "\n",
            "As the new frontier expands, so does the idea of deep exploration.\n",
            "The new frontier is likely to be a very important place for humanity.\n",
            "But that‏ is simply not enough.\n",
            "‏If this is the case, then what are the implications for humanity in the future of humanity‏?\n",
            "As the new frontier expands, so does the idea of deep exploration.\n",
            "Why do we need deeper exploration?\n",
            "We need to explore and discover what goes into that.\n",
            "What if we are all in on the same journey?\n",
            "What if we are all in on the same journey?\n",
            "What if we are all in on the same journey?\n",
            "What if we are all in on the same journey?\n",
            "What if we are all in on the same journey?\n",
            "What if we are all in on the same journey?\n",
            "What if we are all in on the same journey?\n",
            "What if we are all in on the same journey?\n",
            "What if we are all in on the same journey?\n",
            "What if we are all in on the same journey?\n",
            "How would we look at this new frontier?\n",
            "Could we look at this new frontier?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load a pre-trained model pipeline\n",
        "model_name = \"distilgpt2\"  # Smaller, faster version of GPT-2\n",
        "generator = pipeline(\"text-generation\", model=model_name)\n",
        "\n",
        "# Sample dataset: a list of text prompts\n",
        "dataset = [\n",
        "    \"In the beginning, there was only darkness.\",\n",
        "    \"The world of artificial intelligence is evolving rapidly.\",\n",
        "    \"A quick brown fox jumps over the lazy dog.\",\n",
        "    \"Space exploration is the next frontier for humanity.\",\n",
        "]\n",
        "\n",
        "# Generate text for each prompt in the dataset\n",
        "for prompt in dataset:\n",
        "    generated = generator(prompt, max_length=50, num_return_sequences=1)\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Generated Text: {generated[0]['generated_text']}\\n\")\n"
      ]
    }
  ]
}