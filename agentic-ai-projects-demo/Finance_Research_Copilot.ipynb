{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Finance Research Copilot — Agentic AI (LangChain + LangGraph)\n",
        "=============================================================\n",
        "\n",
        "What this is\n",
        "------------\n",
        "An intermediate-complexity **agentic AI application** that orchestrates planning, tool-use, retrieval,\n",
        "analysis, and reporting for an equity/sector research task in **finance** using **LangChain** and **LangGraph**.\n",
        "\n",
        "It demonstrates:\n",
        "- A **planner → router → tool-use → retrieval → analysis → report** loop.\n",
        "- LangGraph **state machine** with typed state and conditional edges.\n",
        "- Tool calling (calculator, simple web search, CSV/PDF loaders) via LangChain Tools.\n",
        "- Local **vector store (Chroma)** with OpenAI embeddings (configurable provider) for RAG.\n",
        "- A **guardrail** self-check pass and a **memory** summary store (JSONL) for continuity.\n",
        "\n",
        "Run it\n",
        "------\n",
        "1) Create a virtualenv and install deps (see `requirements` string near bottom for a ready list):\n",
        "\n",
        "   ```bash\n",
        "   python -m venv .venv && source .venv/bin/activate\n",
        "   pip install -U pip\n",
        "   pip install -r requirements.txt\n",
        "   ```\n",
        "\n",
        "2) Export your LLM/Embeddings key(s):\n",
        "\n",
        "   ```bash\n",
        "   export OPENAI_API_KEY=sk-...  # or set in your shell profile\n",
        "   ```\n",
        "\n",
        "   (You can swap providers in code if you prefer — see notes in `make_llm()` and `make_embeddings()`.)\n",
        "\n",
        "3) Run the app:\n",
        "\n",
        "   ```bash\n",
        "   python agent_finance_copilot.py --query \"Analyze TCS Q1 FY26 results vs Infosys; compute YoY growth and give risks/opportunities\"\n",
        "   ```\n",
        "\n",
        "4) Optional: first run will auto-create a small demo dataset under `./data/` (CSV + MD). You can drop your\n",
        "   own PDFs/CSVs/MD into `./data/` and re-run to enrich retrieval.\n",
        "\n",
        "Outputs\n",
        "-------\n",
        "- A **final research brief** printed to stdout.\n",
        "- A **scratch/plan** section and **tool traces** in the logs.\n",
        "- `./memory/session_memory.jsonl` with rolling summaries.\n",
        "\n",
        "Note\n",
        "----\n",
        "This is a single-file demo for clarity. In production, you’d split into modules and add structured logging,\n",
        "robust evals, and proper error handling.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "iX2hZ6l61-nc",
        "outputId": "8b62f8fb-9f55-453b-ddb0-b5801c3df30c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nFinance Research Copilot — Agentic AI (LangChain + LangGraph)\\n=============================================================\\n\\nWhat this is\\n------------\\nAn intermediate-complexity **agentic AI application** that orchestrates planning, tool-use, retrieval,\\nanalysis, and reporting for an equity/sector research task in **finance** using **LangChain** and **LangGraph**.\\n\\nIt demonstrates:\\n- A **planner → router → tool-use → retrieval → analysis → report** loop.\\n- LangGraph **state machine** with typed state and conditional edges.\\n- Tool calling (calculator, simple web search, CSV/PDF loaders) via LangChain Tools.\\n- Local **vector store (Chroma)** with OpenAI embeddings (configurable provider) for RAG.\\n- A **guardrail** self-check pass and a **memory** summary store (JSONL) for continuity.\\n\\nRun it\\n------\\n1) Create a virtualenv and install deps (see `requirements` string near bottom for a ready list):\\n\\n   ```bash\\n   python -m venv .venv && source .venv/bin/activate\\n   pip install -U pip\\n   pip install -r requirements.txt\\n   ```\\n\\n2) Export your LLM/Embeddings key(s):\\n\\n   ```bash\\n   export OPENAI_API_KEY=sk-...  # or set in your shell profile\\n   ```\\n\\n   (You can swap providers in code if you prefer — see notes in `make_llm()` and `make_embeddings()`.)\\n\\n3) Run the app:\\n\\n   ```bash\\n   python agent_finance_copilot.py --query \"Analyze TCS Q1 FY26 results vs Infosys; compute YoY growth and give risks/opportunities\"\\n   ```\\n\\n4) Optional: first run will auto-create a small demo dataset under `./data/` (CSV + MD). You can drop your\\n   own PDFs/CSVs/MD into `./data/` and re-run to enrich retrieval.\\n\\nOutputs\\n-------\\n- A **final research brief** printed to stdout.\\n- A **scratch/plan** section and **tool traces** in the logs.\\n- `./memory/session_memory.jsonl` with rolling summaries.\\n\\nNote\\n----\\nThis is a single-file demo for clarity. In production, you’d split into modules and add structured logging,\\nrobust evals, and proper error handling.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import sys\n",
        "import textwrap\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional, Tuple, TypedDict"
      ],
      "metadata": {
        "id": "G5bmnywY2I6P"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph\n",
        "!pip install langchain\n",
        "!pip install langchain_community\n",
        "!pip install langchain_core\n",
        "!pip install langchain_text_splitters\n",
        "!pip install duckduckgo_search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmvTOOc02Sy5",
        "outputId": "78d604c1-db23-4331-b5fc-70fbea78e015"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.6.7-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.76)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.11.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.4.28)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.1)\n",
            "Downloading langgraph-0.6.7-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed langgraph-0.6.7 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.9 ormsgpack-1.10.0\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.76)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.28)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.9)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.3.76)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.43)\n",
            "Collecting requests<3,>=2.32.5 (from langchain_community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.6.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.28)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.6.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.6.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (2.11.9)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain_community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain_community) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain_community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain_community) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain_community) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain_community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain_community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain_community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain_community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain_community-0.3.29 marshmallow-3.26.1 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.12/dist-packages (0.3.76)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (0.4.28)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (2.11.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain_core) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain_core) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain_core) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain_core) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain_core) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain_core) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain_core) (0.4.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain_core) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain_core) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core) (1.3.1)\n",
            "Requirement already satisfied: langchain_text_splitters in /usr/local/lib/python3.12/dist-packages (0.3.11)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain_text_splitters) (0.3.76)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (0.4.28)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (2.11.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (0.4.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters) (1.3.1)\n",
            "Collecting duckduckgo_search\n",
            "  Downloading duckduckgo_search-8.1.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from duckduckgo_search) (8.2.1)\n",
            "Collecting primp>=0.15.0 (from duckduckgo_search)\n",
            "  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from duckduckgo_search) (5.4.0)\n",
            "Downloading duckduckgo_search-8.1.1-py3-none-any.whl (18 kB)\n",
            "Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: primp, duckduckgo_search\n",
            "Successfully installed duckduckgo_search-8.1.1 primp-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa78_qNI3OW8",
        "outputId": "14610ea1-217a-438d-fe00-78917f4925ff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.33-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.76 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.3.76)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.108.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.11.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (0.4.28)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (2.11.9)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.5)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.104.2->langchain_openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain_openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain_openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain_openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.76->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_openai) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_openai) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain_openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain_openai) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.5.0)\n",
            "Downloading langchain_openai-0.3.33-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_openai\n",
            "Successfully installed langchain_openai-0.3.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF0n-hypJApj",
        "outputId": "03d30f97-b733-4a82-8871-d34b5d3d2a8c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-1.1.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.9)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.75.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.17.4)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.5)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.37.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.37.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.37.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.58b0)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.35.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.10)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.1.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.37.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=966fe3d586d03e52640d92b3ca308b7d7412add6581fe3f029e270eae6caf489\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, pybase64, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, posthog, opentelemetry-exporter-otlp-proto-common, coloredlogs, onnxruntime, kubernetes, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.1.0 coloredlogs-15.0.1 durationpy-0.10 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 mmh3-5.2.0 onnxruntime-1.22.1 opentelemetry-exporter-otlp-proto-common-1.37.0 opentelemetry-exporter-otlp-proto-grpc-1.37.0 opentelemetry-proto-1.37.0 posthog-5.4.0 pybase64-1.4.2 pypika-0.48.9 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Retrieve the API key from Colab secrets\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Set the environment variable\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key"
      ],
      "metadata": {
        "id": "kfImuWrgJBZK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- LangChain / LangGraph imports ---\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "# LLMs / Embeddings providers\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "# Vector store (Chroma)\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.document_loaders import TextLoader, CSVLoader, PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Optional utility\n",
        "from duckduckgo_search import DDGS  # lightweight search (no API key)"
      ],
      "metadata": {
        "id": "nDLzIaH32OeZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Config & Utilities\n",
        "# -----------------------\n",
        "import os\n",
        "import random # Import random\n",
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path(os.getcwd())\n",
        "DATA_DIR = ROOT / \"data\"\n",
        "DB_DIR = ROOT / \"chroma_db\"\n",
        "MEM_DIR = ROOT / \"memory\"\n",
        "MEM_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MEM_FILE = MEM_DIR / \"session_memory.jsonl\"\n",
        "\n",
        "RANDOM_SEED = 37\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "\n",
        "def hrule(title: str = \"\") -> str:\n",
        "    line = \"\\n\" + (\"=\" * 80)\n",
        "    return f\"{line}\\n{title}\\n{line}\\n\""
      ],
      "metadata": {
        "id": "deCm__7W4LLY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# LLMs & Embeddings\n",
        "# -----------------------\n",
        "\n",
        "def make_llm(model: str = \"gpt-4o-mini\", temperature: float = 0.2):\n",
        "    \"\"\"Return an LLM. Swap provider here if needed.\n",
        "    - For OpenAI: set OPENAI_API_KEY in environment.\n",
        "    - You could adapt this to Azure OpenAI (langchain-openai supports it) or Anthropic (use langchain-anthropic).\n",
        "    \"\"\"\n",
        "    return ChatOpenAI(model=model, temperature=temperature)\n",
        "\n",
        "\n",
        "def make_embeddings(model: str = \"text-embedding-3-large\"):\n",
        "    return OpenAIEmbeddings(model=model)\n"
      ],
      "metadata": {
        "id": "8kSqcnGkF6aF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Demo Data Bootstrap\n",
        "# -----------------------\n",
        "\n",
        "def bootstrap_demo_corpus() -> None:\n",
        "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    # Simple CSV with quarterly numbers (illustrative synthetic values)\n",
        "    csv_path = DATA_DIR / \"it_services_q_results.csv\"\n",
        "    if not csv_path.exists():\n",
        "        csv_path.write_text(\n",
        "            \"\"\"company,quarter,fy,rev_inr_cr,profit_inr_cr,yoy_rev_growth_pct\n",
        "TCS,Q1,26,64000,12800,7.5\n",
        "Infosys,Q1,26,38000,7200,5.2\n",
        "HCLTech,Q1,26,28000,4200,6.1\n",
        "Wipro,Q1,26,22000,3100,3.9\n",
        "\"\"\"\n",
        "        )\n",
        "\n",
        "    # Short markdown notes to be retrievable\n",
        "    md_path = DATA_DIR / \"sector_notes.md\"\n",
        "    if not md_path.exists():\n",
        "        md_path.write_text(\n",
        "            textwrap.dedent(\n",
        "                \"\"\"\n",
        "                # India IT Services Sector — Quick Notes (FY26 Q1)\n",
        "\n",
        "                - Demand steady in BFSI and healthcare; telecom muted.\n",
        "                - Cost optimization continues; vendor consolidation favors top-3 players.\n",
        "                - GenAI pilots moving to production in customer support and code modernization.\n",
        "                - Currency tailwinds mixed; cross-currency impact ~(-0.4%) for Q1.\n",
        "                - Risks: prolonged US slowdown, pricing pressure, large deal ramp-downs.\n",
        "                - Opportunities: cloud modernization, vendor consolidation, GenAI productivity deals.\n",
        "                \"\"\"\n",
        "            ).strip()\n",
        "        )\n",
        "\n",
        "    # Tiny PDF (single-page) with a mock excerpt\n",
        "    pdf_path = DATA_DIR / \"mock_investor_update.pdf\"\n",
        "    if not pdf_path.exists():\n",
        "        # Generate a simple PDF from text (fallback as .txt if PyPDF2 not present) — but we'll ship as a text stub\n",
        "        # so loader can still demonstrate. For reliability, just create a text that we load via TextLoader.\n",
        "        (DATA_DIR / \"mock_investor_update.txt\").write_text(\n",
        "            \"Investor Update: Tier-1 IT firms report stable margins; deal pipeline healthy; GenAI backlog building.\"\n",
        "        )\n",
        "\n"
      ],
      "metadata": {
        "id": "vJaKFKA3F_Yz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Document Ingestion & Vector Store\n",
        "# -----------------------\n",
        "from typing import List, Any # Import List and Any\n",
        "\n",
        "def load_documents() -> List[Any]:\n",
        "    docs: List[Any] = []\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)\n",
        "    for path in DATA_DIR.glob(\"**/*\"):\n",
        "        if path.is_dir():\n",
        "            continue\n",
        "        try:\n",
        "            if path.suffix.lower() in {\".md\", \".txt\"}:\n",
        "                loader = TextLoader(str(path))\n",
        "                docs.extend(splitter.split_documents(loader.load()))\n",
        "            elif path.suffix.lower() == \".csv\":\n",
        "                loader = CSVLoader(str(path))\n",
        "                docs.extend(splitter.split_documents(loader.load()))\n",
        "            elif path.suffix.lower() == \".pdf\":\n",
        "                # If you add a real PDF, this loader will work\n",
        "                loader = PyPDFLoader(str(path))\n",
        "                docs.extend(splitter.split_documents(loader.load()))\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Skipping {path.name}: {e}\")\n",
        "    return docs\n",
        "\n",
        "\n",
        "def build_or_load_vectorstore(emb_model: str = \"text-embedding-3-large\") -> Chroma:\n",
        "    embeddings = make_embeddings(model=emb_model)\n",
        "    if DB_DIR.exists() and any(DB_DIR.iterdir()):\n",
        "        db = Chroma(collection_name=\"finance_copilot\", persist_directory=str(DB_DIR), embedding_function=embeddings)\n",
        "    else:\n",
        "        docs = load_documents()\n",
        "        db = Chroma.from_documents(\n",
        "            docs,\n",
        "            embeddings,\n",
        "            collection_name=\"finance_copilot\",\n",
        "            persist_directory=str(DB_DIR),\n",
        "        )\n",
        "    return db"
      ],
      "metadata": {
        "id": "ODxVb6E2GDWw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# LangChain Tools\n",
        "# -----------------------\n",
        "\n",
        "@tool(\"calc\")\n",
        "def calc(expression: str) -> str:\n",
        "    \"\"\"Safely evaluate a simple math expression. Supports +,-,*,/,**,(), and decimals.\n",
        "    Example: \"(64000-38000)/38000\".\n",
        "    \"\"\"\n",
        "    # Basic safety: restrict to allowed characters\n",
        "    if not re.fullmatch(r\"[0-9+\\-*/(). %**]+\", expression.replace(\" \", \"\")):\n",
        "        return \"Error: unsupported characters in expression.\"\n",
        "    try:\n",
        "        # Evaluate with restricted globals\n",
        "        value = eval(expression, {\"__builtins__\": {}}, {\"math\": math})\n",
        "        return str(value)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "\n",
        "@tool(\"ddg_news\")\n",
        "def ddg_news(query: str, max_results: int = 5) -> str:\n",
        "    \"\"\"Lightweight web search via DuckDuckGo for recent news headlines/snippets.\n",
        "    Returns a JSON string list of {title, href, snippet}.\n",
        "    \"\"\"\n",
        "    results: List[Dict[str, str]] = []\n",
        "    try:\n",
        "        with DDGS() as ddgs:\n",
        "            for r in ddgs.news(query, max_results=max_results):\n",
        "                results.append({\"title\": r.get(\"title\", \"\"), \"href\": r.get(\"url\", \"\"), \"snippet\": r.get(\"body\", \"\")})\n",
        "    except Exception as e:\n",
        "        results.append({\"title\": \"(search error)\", \"href\": \"\", \"snippet\": str(e)})\n",
        "    return json.dumps(results, ensure_ascii=False)\n",
        "\n",
        "\n",
        "@tool(\"tabular_lookup\")\n",
        "def tabular_lookup(company: str, quarter: str = \"Q1\", fy: str = \"26\") -> str:\n",
        "    \"\"\"Look up synthetic quarterly metrics from the demo CSV. Returns a JSON dict with fields\n",
        "    {company, quarter, fy, rev_inr_cr, profit_inr_cr, yoy_rev_growth_pct} if found, else {}.\n",
        "    \"\"\"\n",
        "    csv_path = DATA_DIR / \"it_services_q_results.csv\"\n",
        "    if not csv_path.exists():\n",
        "        return json.dumps({})\n",
        "    rows = [line.strip().split(\",\") for line in csv_path.read_text().splitlines()]\n",
        "    header = rows[0]\n",
        "    for r in rows[1:]:\n",
        "        row = dict(zip(header, r))\n",
        "        if row[\"company\"].lower() == company.lower() and row[\"quarter\"].upper() == quarter.upper() and row[\"fy\"] == fy:\n",
        "            return json.dumps(row)\n",
        "    return json.dumps({})\n",
        "\n",
        "\n",
        "TOOLS = [calc, ddg_news, tabular_lookup]\n",
        "TOOL_NODE = ToolNode(tools=TOOLS)\n",
        "\n"
      ],
      "metadata": {
        "id": "me9nbCyFGQkU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nJh8oZrj13UI"
      },
      "outputs": [],
      "source": [
        "# -----------------------\n",
        "# LangGraph State\n",
        "# -----------------------\n",
        "from typing import List, Any, TypedDict # Import TypedDict\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    query: str\n",
        "    plan: str\n",
        "    context_snippets: List[str]\n",
        "    tool_calls: List[str]\n",
        "    analysis: str\n",
        "    report: str\n",
        "    guard_feedback: str\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Nodes\n",
        "# -----------------------\n",
        "\n",
        "def node_planner(state: GraphState) -> GraphState:\n",
        "    llm = make_llm()\n",
        "    sys_prompt = (\n",
        "        \"You are a senior equity research analyst. Break the user query into a short numbered plan: \"\n",
        "        \"1) clarify intent (if needed), 2) data to fetch (tools), 3) retrieval queries, 4) calculations, \"\n",
        "        \"5) risks/opportunities, 6) output format with headings. Keep it under 120 words.\"\n",
        "    )\n",
        "    msgs = [SystemMessage(content=sys_prompt), HumanMessage(content=state[\"query\"])]\n",
        "    out = llm.invoke(msgs)\n",
        "    return {**state, \"plan\": out.content}\n",
        "\n",
        "\n",
        "def node_router(state: GraphState) -> GraphState:\n",
        "    # A tiny heuristic router: decide whether we need tools or retrieval based on keywords\n",
        "    q = state[\"query\"].lower()\n",
        "    needs_news = any(k in q for k in [\"news\", \"latest\", \"today\", \"headline\"]) or \"vs\" in q\n",
        "    needs_calc = any(k in q for k in [\"growth\", \"cagr\", \"difference\", \"%\", \"yoy\", \"compute\", \"calculate\"])\n",
        "    needs_tabular = any(k in q for k in [\"revenue\", \"profit\", \"q1\", \"fy26\", \"results\", \"numbers\"]) or \"vs\" in q\n",
        "\n",
        "    tool_calls = []\n",
        "    if needs_news:\n",
        "        tool_calls.append(\"ddg_news\")\n",
        "    if needs_calc:\n",
        "        tool_calls.append(\"calc\")\n",
        "    if needs_tabular:\n",
        "        tool_calls.append(\"tabular_lookup\")\n",
        "\n",
        "    return {**state, \"tool_calls\": tool_calls}\n",
        "\n",
        "\n",
        "def node_retrieve(state: GraphState) -> GraphState:\n",
        "    db = build_or_load_vectorstore()\n",
        "    retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
        "    # Build a synthetic retrieval query from the plan + original query\n",
        "    q = f\"{state['query']}\\nContext needed: sector risks, demand trends, and genAI themes from notes\"\n",
        "    docs = retriever.get_relevant_documents(q)\n",
        "    snippets = [f\"[{d.metadata.get('source','')}] {d.page_content[:400]}\" for d in docs]\n",
        "    return {**state, \"context_snippets\": snippets}\n",
        "\n",
        "\n",
        "def node_tool_use(state: GraphState) -> GraphState:\n",
        "    # Let the LLM decide how to call tools using function-calling, given our Tools list.\n",
        "    llm = make_llm()\n",
        "    llm_with_tools = llm.bind_tools(TOOLS)\n",
        "\n",
        "    tool_context = (\n",
        "        \"Available tools: calc(expression), ddg_news(query, max_results=5), tabular_lookup(company, quarter, fy).\\n\"\n",
        "        \"When comparing companies (e.g., 'A vs B'), call tabular_lookup for each to fetch numbers, then calc for ratios.\"\n",
        "    )\n",
        "\n",
        "    msgs = [\n",
        "        SystemMessage(content=tool_context + \" Return concise JSON per tool call, then a 2-3 line interim note.\"),\n",
        "        HumanMessage(content=f\"User query: {state['query']}\\nPlan: {state['plan']}\")\n",
        "    ]\n",
        "\n",
        "    # First pass: ask model if it wants to call tools\n",
        "    first = llm_with_tools.invoke(msgs)\n",
        "    tool_traces: List[str] = []\n",
        "    tool_msgs: List[Any] = []\n",
        "\n",
        "    if hasattr(first, \"tool_calls\") and first.tool_calls:\n",
        "        for tc in first.tool_calls:\n",
        "            name = tc[\"name\"]\n",
        "            args = tc.get(\"args\", {})\n",
        "            # Execute tool\n",
        "            result = None\n",
        "            if name == \"calc\":\n",
        "                result = calc.invoke(args)\n",
        "            elif name == \"ddg_news\":\n",
        "                result = ddg_news.invoke(args)\n",
        "            elif name == \"tabular_lookup\":\n",
        "                result = tabular_lookup.invoke(args)\n",
        "            tool_traces.append(f\"TOOL {name}({args}) -> {result[:240]}...\")\n",
        "            tool_msgs.append(ToolMessage(tool_call_id=tc[\"id\"], name=name, content=str(result)))\n",
        "\n",
        "    # Second pass: give tool outputs back to model\n",
        "    follow = llm.invoke(msgs + [first] + tool_msgs)\n",
        "    interim = follow.content\n",
        "\n",
        "    return {**state, \"analysis\": interim, \"tool_calls\": state[\"tool_calls\"] + tool_traces}\n",
        "\n",
        "\n",
        "def node_analyze_and_write(state: GraphState) -> GraphState:\n",
        "    llm = make_llm(temperature=0.2)\n",
        "    sys_prompt = (\n",
        "        \"Compose a crisp equity research brief with sections: SUMMARY, KEY METRICS, DRIVERS, RISKS, OPPORTUNITIES, \"\n",
        "        \"ACTIONABLE INSIGHTS. Use bullet points, cite any numbers you computed, and keep to ~300-450 words.\"\n",
        "    )\n",
        "    ctx = \"\\n\\n\".join(state.get(\"context_snippets\", []))\n",
        "    tool_log = \"\\n\".join(state.get(\"tool_calls\", []))\n",
        "\n",
        "    msgs = [\n",
        "        SystemMessage(content=sys_prompt),\n",
        "        HumanMessage(\n",
        "            content=f\"User query: {state['query']}\\n\\nContext from RAG:\\n{ctx}\\n\\nTool trace:\\n{tool_log}\\n\\nDo the final brief now.\"\n",
        "        ),\n",
        "    ]\n",
        "    out = llm.invoke(msgs)\n",
        "    return {**state, \"report\": out.content}\n",
        "\n",
        "\n",
        "def node_guardrail(state: GraphState) -> GraphState:\n",
        "    llm = make_llm(temperature=0)\n",
        "    sys_prompt = (\n",
        "        \"You are a meticulous research QA assistant. Check the brief for: unsupported claims, unclear sources, \"\n",
        "        \"and missing assumptions. Reply with a short bullet list of corrections or 'LGTM' if fine.\"\n",
        "    )\n",
        "    msgs = [SystemMessage(content=sys_prompt), HumanMessage(content=state[\"report\"])]\n",
        "    fb = llm.invoke(msgs).content\n",
        "    return {**state, \"guard_feedback\": fb}\n",
        "\n",
        "\n",
        "def node_memory(state: GraphState) -> GraphState:\n",
        "    rec = {\n",
        "        \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
        "        \"query\": state[\"query\"],\n",
        "        \"plan\": state.get(\"plan\", \"\"),\n",
        "        \"key_points\": state.get(\"analysis\", \"\")[:800],\n",
        "        \"summary\": state.get(\"report\", \"\")[:1200],\n",
        "    }\n",
        "    with open(MEM_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "    return state\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Graph Wiring\n",
        "# -----------------------\n",
        "\n",
        "def build_graph():\n",
        "    g = StateGraph(GraphState)\n",
        "    g.add_node(\"planner\", node_planner)\n",
        "    g.add_node(\"router\", node_router)\n",
        "    g.add_node(\"retrieve\", node_retrieve)\n",
        "    g.add_node(\"tool_use\", node_tool_use)\n",
        "    g.add_node(\"write\", node_analyze_and_write)\n",
        "    g.add_node(\"guard\", node_guardrail)\n",
        "    g.add_node(\"memory\", node_memory)\n",
        "\n",
        "    g.set_entry_point(\"planner\")\n",
        "    g.add_edge(\"planner\", \"router\")\n",
        "\n",
        "    # Always retrieve some context\n",
        "    g.add_edge(\"router\", \"retrieve\")\n",
        "\n",
        "    # If router decided tools are relevant, still run the tool node — it will decide if any tool calls happen.\n",
        "    g.add_edge(\"retrieve\", \"tool_use\")\n",
        "\n",
        "    # Then synthesize report\n",
        "    g.add_edge(\"tool_use\", \"write\")\n",
        "\n",
        "    # Guardrail check, then persist memory and end\n",
        "    g.add_edge(\"write\", \"guard\")\n",
        "    g.add_edge(\"guard\", \"memory\")\n",
        "    g.add_edge(\"memory\", END)\n",
        "\n",
        "    return g.compile()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# CLI Entrypoint\n",
        "# -----------------------\n",
        "import argparse\n",
        "\n",
        "def main(query: str = \"Analyze TCS Q1 FY26 results vs Infosys; compute YoY growth and give risks/opportunities\"):\n",
        "    parser = argparse.ArgumentParser(description=\"Finance Research Copilot — Agentic AI (LangChain + LangGraph)\")\n",
        "    parser.add_argument(\"--query\", required=False, help=\"User research question / task\")\n",
        "    args, unknown = parser.parse_known_args() # Use parse_known_args to ignore unknown arguments\n",
        "\n",
        "    # Use the provided query argument if available, otherwise use the default\n",
        "    user_query = args.query if args.query is not None else query\n",
        "\n",
        "    bootstrap_demo_corpus()\n",
        "\n",
        "    app = build_graph()\n",
        "    initial: GraphState = {\n",
        "        \"query\": user_query,\n",
        "        \"plan\": \"\",\n",
        "        \"context_snippets\": [],\n",
        "        \"tool_calls\": [],\n",
        "        \"analysis\": \"\",\n",
        "        \"report\": \"\",\n",
        "        \"guard_feedback\": \"\",\n",
        "    }\n",
        "\n",
        "    print(hrule(\"START RUN\"))\n",
        "    final: GraphState = app.invoke(initial)\n",
        "\n",
        "    print(hrule(\"PLAN\"))\n",
        "    print(final.get(\"plan\", \"\"))\n",
        "\n",
        "    print(hrule(\"TOOL TRACE (truncated)\"))\n",
        "    for t in final.get(\"tool_calls\", [])[:12]:\n",
        "        print(\"-\", t)\n",
        "\n",
        "    print(hrule(\"CONTEXT SNIPPETS\"))\n",
        "    for s in final.get(\"context_snippets\", [])[:4]:\n",
        "        print(\"*\", s[:240], \"...\")\n",
        "\n",
        "    print(hrule(\"RESEARCH BRIEF\"))\n",
        "    print(final.get(\"report\", \"\"))\n",
        "\n",
        "    print(hrule(\"GUARDRAIL FEEDBACK\"))\n",
        "    print(final.get(\"guard_feedback\", \"\"))\n",
        "\n",
        "    print(hrule(\"DONE\"))"
      ],
      "metadata": {
        "id": "3AdYw0Th38he"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # If running as a script, we also emit a handy requirements.txt next to the file\n",
        "    import textwrap # Import textwrap\n",
        "    requirements = textwrap.dedent(\n",
        "        \"\"\"\n",
        "        # Core\n",
        "        langchain>=0.2.11\n",
        "        langgraph>=0.2.13\n",
        "        langchain-openai>=0.1.23\n",
        "        langchain-community>=0.2.10\n",
        "        langchain-text-splitters>=0.2.2\n",
        "\n",
        "        # Vector store\n",
        "        chromadb>=0.5.3\n",
        "\n",
        "        # Loaders\n",
        "        pypdf>=4.2.0\n",
        "\n",
        "        # Optional util\n",
        "        duckduckgo-search>=6.2.10\n",
        "\n",
        "        # Misc\n",
        "        tiktoken>=0.7.0\n",
        "        numpy>=1.26.0\n",
        "        pandas>=2.2.2\n",
        "        \"\"\"\n",
        "    ).strip()\n",
        "\n",
        "    req_path = ROOT / \"requirements.txt\"\n",
        "    try:\n",
        "        if not req_path.exists():\n",
        "            req_path.write_text(requirements)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not write requirements.txt: {e}\")"
      ],
      "metadata": {
        "id": "xoaPcngl3xTf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCAYfzJk30PH",
        "outputId": "65387a2a-51cb-43e9-c560-094f4b43b277"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "START RUN\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-786429608.py:55: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = retriever.get_relevant_documents(q)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "PLAN\n",
            "\n",
            "================================================================================\n",
            "\n",
            "1) Clarify Intent: Confirm if the user wants a comparative analysis of TCS and Infosys Q1 FY26 results, focusing on YoY growth.\n",
            "\n",
            "2) Data to Fetch: Financial statements for TCS and Infosys Q1 FY26, including revenue, net income, and key metrics.\n",
            "\n",
            "3) Retrieval Queries: Search for TCS Q1 FY26 results and Infosys Q1 FY26 results from financial databases or company filings.\n",
            "\n",
            "4) Calculations: Compute YoY growth rates for revenue and net income for both companies.\n",
            "\n",
            "5) Risks/Opportunities: Identify potential risks (e.g., market competition, regulatory changes) and opportunities (e.g., new markets, technological advancements) for both companies.\n",
            "\n",
            "6) Output Format: \n",
            "   - Title: Comparative Analysis of TCS and Infosys Q1 FY26 Results\n",
            "   - Section 1: YoY Growth Comparison\n",
            "   - Section 2: Risks\n",
            "   - Section 3: Opportunities\n",
            "\n",
            "================================================================================\n",
            "TOOL TRACE (truncated)\n",
            "\n",
            "================================================================================\n",
            "\n",
            "- ddg_news\n",
            "- calc\n",
            "- tabular_lookup\n",
            "- TOOL tabular_lookup({'company': 'TCS', 'quarter': 'Q1', 'fy': '26'}) -> {\"company\": \"TCS\", \"quarter\": \"Q1\", \"fy\": \"26\", \"rev_inr_cr\": \"64000\", \"profit_inr_cr\": \"12800\", \"yoy_rev_growth_pct\": \"7.5\"}...\n",
            "- TOOL tabular_lookup({'company': 'Infosys', 'quarter': 'Q1', 'fy': '26'}) -> {\"company\": \"Infosys\", \"quarter\": \"Q1\", \"fy\": \"26\", \"rev_inr_cr\": \"38000\", \"profit_inr_cr\": \"7200\", \"yoy_rev_growth_pct\": \"5.2\"}...\n",
            "\n",
            "================================================================================\n",
            "CONTEXT SNIPPETS\n",
            "\n",
            "================================================================================\n",
            "\n",
            "* [/content/data/sector_notes.md] # India IT Services Sector — Quick Notes (FY26 Q1)\n",
            "\n",
            "- Demand steady in BFSI and healthcare; telecom muted.\n",
            "- Cost optimization continues; vendor consolidation favors top-3 players.\n",
            "- GenAI pilots moving to pr ...\n",
            "* [/content/data/it_services_q_results.csv] company: TCS\n",
            "quarter: Q1\n",
            "fy: 26\n",
            "rev_inr_cr: 64000\n",
            "profit_inr_cr: 12800\n",
            "yoy_rev_growth_pct: 7.5 ...\n",
            "* [/content/data/mock_investor_update.txt] Investor Update: Tier-1 IT firms report stable margins; deal pipeline healthy; GenAI backlog building. ...\n",
            "* [/content/data/it_services_q_results.csv] company: Infosys\n",
            "quarter: Q1\n",
            "fy: 26\n",
            "rev_inr_cr: 38000\n",
            "profit_inr_cr: 7200\n",
            "yoy_rev_growth_pct: 5.2 ...\n",
            "\n",
            "================================================================================\n",
            "RESEARCH BRIEF\n",
            "\n",
            "================================================================================\n",
            "\n",
            "**EQUITY RESEARCH BRIEF: TCS vs Infosys Q1 FY26 Results**\n",
            "\n",
            "**SUMMARY**  \n",
            "TCS reported a robust performance in Q1 FY26, achieving a revenue of ₹64,000 crore and a profit of ₹12,800 crore, reflecting a YoY growth of 7.5%. In contrast, Infosys generated ₹38,000 crore in revenue with a profit of ₹7,200 crore, marking a YoY growth of 5.2%. Both companies are navigating a steady demand environment, particularly in BFSI and healthcare, but face challenges in telecom and pricing pressures.\n",
            "\n",
            "**KEY METRICS**  \n",
            "- **TCS**:  \n",
            "  - Revenue: ₹64,000 crore  \n",
            "  - Profit: ₹12,800 crore  \n",
            "  - YoY Revenue Growth: 7.5%  \n",
            "- **Infosys**:  \n",
            "  - Revenue: ₹38,000 crore  \n",
            "  - Profit: ₹7,200 crore  \n",
            "  - YoY Revenue Growth: 5.2%  \n",
            "\n",
            "**DRIVERS**  \n",
            "- **Demand Stability**: Continued demand in BFSI and healthcare sectors supports revenue growth.\n",
            "- **Cost Optimization**: Ongoing vendor consolidation favors top-tier players like TCS and Infosys.\n",
            "- **GenAI Integration**: Transition from pilot projects to production in customer support and code modernization enhances service offerings.\n",
            "\n",
            "**RISKS**  \n",
            "- **US Economic Slowdown**: Prolonged economic challenges in the US could dampen demand.\n",
            "- **Pricing Pressure**: Increased competition may lead to reduced pricing power.\n",
            "- **Large Deal Ramp-downs**: Potential slowdown in large contracts could impact revenue streams.\n",
            "\n",
            "**OPPORTUNITIES**  \n",
            "- **GenAI Backlog**: Growing interest in GenAI solutions presents opportunities for new service lines and revenue streams.\n",
            "- **Market Consolidation**: Vendor consolidation may allow top players to capture greater market share.\n",
            "- **Diversification**: Expanding into emerging markets and sectors can mitigate risks associated with traditional revenue streams.\n",
            "\n",
            "**ACTIONABLE INSIGHTS**  \n",
            "- **Investment Recommendation**: TCS shows stronger growth metrics and profitability compared to Infosys, making it a more attractive investment option in the current environment.\n",
            "- **Monitor Economic Indicators**: Keep an eye on US economic trends and their potential impact on IT spending.\n",
            "- **Focus on GenAI**: Both companies should prioritize investments in GenAI capabilities to leverage the growing demand for AI-driven solutions.\n",
            "\n",
            "In conclusion, while TCS outperforms Infosys in Q1 FY26, both companies face similar risks and opportunities that could shape their future performance. Investors should weigh these factors carefully when considering positions in the Indian IT services sector.\n",
            "\n",
            "================================================================================\n",
            "GUARDRAIL FEEDBACK\n",
            "\n",
            "================================================================================\n",
            "\n",
            "- **Unsupported Claims**: The brief states \"both companies are navigating a steady demand environment\" without providing data or sources to support this assertion. It would be beneficial to include specific market analysis or reports that substantiate this claim.\n",
            "- **Unclear Sources**: The metrics provided (revenue, profit, YoY growth) lack citations or references to the source of this financial data. It is important to specify where this information was obtained (e.g., company earnings reports, financial news articles).\n",
            "- **Missing Assumptions**: The brief does not clarify the assumptions behind the projections for demand stability and the impact of GenAI integration. It would be helpful to outline the basis for these assumptions, such as market trends or historical performance data.\n",
            "\n",
            "Overall, the brief could benefit from additional citations and clarification of assumptions to strengthen its credibility.\n",
            "\n",
            "================================================================================\n",
            "DONE\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-786429608.py:134: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
          ]
        }
      ]
    }
  ]
}